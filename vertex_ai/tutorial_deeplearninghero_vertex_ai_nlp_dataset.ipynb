{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/tutorials_deeplearninghero/blob/master/vertex_ai/tutorial_deeplearninghero_vertex_ai_nlp_dataset.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"/>Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpzEwgk81wlO"
   },
   "source": [
    "## Authenticating the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "aaOV2FVmpwIc"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-hYB1D112E3"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ea9G1YFrpCfO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "PROJECT_ID = \"\" # Set this to your project name\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}-imdb-review-dataset\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "if not PROJECT_ID:\n",
    "    raise ValueError(\"You must set a non-empty PROJECT_ID and make sure the project is created in GCP\") \n",
    "    \n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"BUCKET_URI\"] = BUCKET_URI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N6-TP_52B4g"
   },
   "source": [
    "## Download and extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndc7-c7No_Xa",
    "outputId": "e265cf95-a75d-416b-e347-53231dfc460c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data\n"
     ]
    }
   ],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "def maybe_download(url:str, download_dir:str=\"data\"):\n",
    "    \"\"\" Download the dataset from the web and extract the data \"\"\"\n",
    "    _, filename = os.path.split(url)\n",
    "    download_path = os.path.join(download_dir, filename)\n",
    "\n",
    "    if not os.path.exists(download_path):\n",
    "        print(\"Downloading data\")\n",
    "        response = requests.get(url)\n",
    "        os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "        with open(download_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(\"File is already downloaded\")\n",
    "\n",
    "    tar = tarfile.open(download_path, \"r:gz\")\n",
    "    tar.extractall(os.path.join(download_dir,\"unzip\"))\n",
    "    tar.close()\n",
    "\n",
    "maybe_download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHmV0Tx62E-G"
   },
   "source": [
    "## Sampling a small subset from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BdgKOfpzp9oW"
   },
   "outputs": [],
   "source": [
    "def generate_truncated_dataset(dataset_dir:str, n:int=2000):\n",
    "    \"\"\"It may take painstakenly long time to create a vertex AI dataset.\n",
    "    Therefore we're going to limit the data we'll be using\"\"\"\n",
    "\n",
    "    n_per_label = n//2\n",
    "    parent_dir, _ = os.path.split(dataset_dir)\n",
    "    truncated_dir = os.path.join(parent_dir, \"unzip_truncated\")\n",
    "    os.makedirs(truncated_dir, exist_ok=True)\n",
    "\n",
    "    for sub_dir in [\n",
    "        os.path.join(\"aclImdb\", \"train\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"train\", \"pos\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"neg\"), \n",
    "        os.path.join(\"aclImdb\", \"test\", \"pos\")\n",
    "    ]:\n",
    "\n",
    "        os.makedirs(os.path.join(truncated_dir, sub_dir), exist_ok=True)\n",
    "        for f in os.listdir(os.path.join(dataset_dir, sub_dir)):\n",
    "            sample_id = int(f.split(\"_\")[0])\n",
    "            if sample_id < n_per_label:\n",
    "                shutil.copy(os.path.join(dataset_dir, sub_dir, f), os.path.join(truncated_dir, sub_dir, f))\n",
    "\n",
    "generate_truncated_dataset(os.path.join(\"data/unzip\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_snKJCF2Kc2"
   },
   "source": [
    "## Preparing the data for Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7K0184VCqEld",
    "outputId": "2e0c25c3-d69d-4fee-8515-8a12a1257125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"textContent\": \"some review text\",\n",
      "    \"classificationAnnotation\": {\n",
      "        \"displayName\": \"positive\"\n",
      "    },\n",
      "    \"dataItemResourceLabels\": {\n",
      "        \"aiplatform.googleapis.com/ml_use\": \"training\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pydantic \n",
    "from typing import Dict,List,Literal, Tuple\n",
    "\n",
    "# Format from https://cloud.google.com/vertex-ai/docs/text-data/classification/prepare-data\n",
    "class ClassificationAnnotation(pydantic.BaseModel):\n",
    "    displayName: Literal[\"positive\", \"negative\"]\n",
    "\n",
    "class DataItemResourceLabels(pydantic.BaseModel):\n",
    "    ml_use: Literal[\"training\", \"validation\", \"test\"] = pydantic.Field(alias=\"aiplatform.googleapis.com/ml_use\")\n",
    "    # Enables us to use ml_use=<x> instead of the long field name\n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "\n",
    "class TextClassificationSample(pydantic.BaseModel):\n",
    "    textContent: str\n",
    "    classificationAnnotation: ClassificationAnnotation \n",
    "    dataItemResourceLabels: DataItemResourceLabels\n",
    "\n",
    "instance = TextClassificationSample(\n",
    "    textContent=\"some review text\", \n",
    "    classificationAnnotation=ClassificationAnnotation(displayName=\"positive\"),\n",
    "    dataItemResourceLabels=DataItemResourceLabels(ml_use=\"training\")\n",
    ")\n",
    "\n",
    "print(instance.json(by_alias=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2dYCkSrqWIU",
    "outputId": "72c98ef2-3c46-46ba-d8d6-3222dcd9be6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from the GCS bucket\n",
      "\tFound 2000 train instances\n",
      "Reading test data from the GCS bucket\n",
      "\tFound 1013 validation instances and 987 test instances\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage \n",
    "random.seed(946021)\n",
    "\n",
    "\n",
    "def read_data(file_path:str) -> str:\n",
    "    \"\"\" Read a text file from a given path \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Solving the non-interchangeable valid content error during data import\n",
    "    data_processed = data.replace(\"\\u0085\", \" \").replace(\"\\u0096\", \" \")\n",
    "    return data_processed\n",
    "\n",
    "def generate_single_instance(file_path:str, ml_use:str) -> TextClassificationSample:\n",
    "    \"\"\" Given a filepath, create a single TextClassificationSample instance \"\"\"\n",
    "    label = None\n",
    "    if \"pos\" in file_path:\n",
    "        label = \"positive\"\n",
    "    elif \"neg\" in file_path:\n",
    "        label = \"negative\"\n",
    "    if label:\n",
    "        instance = TextClassificationSample(\n",
    "            textContent=read_data(file_path), \n",
    "            classificationAnnotation=ClassificationAnnotation(displayName=label),\n",
    "            dataItemResourceLabels=DataItemResourceLabels(ml_use=ml_use)\n",
    "        )\n",
    "        return instance\n",
    "    else:\n",
    "        raise ValueError(\"label cannot be None\")\n",
    "\n",
    "def create_instances(data_dir: str) -> Tuple[List[TextClassificationSample], List[TextClassificationSample]]:\n",
    "\n",
    "    train_subdir = \"train\"\n",
    "    test_subdir = \"test\"\n",
    "    train_instances = []\n",
    "\n",
    "    print(f\"Reading training data from the GCS bucket\")\n",
    "    for root, _, files in os.walk(os.path.join(data_dir, train_subdir)):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            if fpath.endswith(\".txt\"):\n",
    "                instance = generate_single_instance(\n",
    "                    file_path=fpath, ml_use=\"training\"\n",
    "                )\n",
    "                train_instances.append(instance.json(by_alias=True, ensure_ascii=False)+'\\n')\n",
    "\n",
    "    print(f\"\\tFound {len(train_instances)} train instances\")\n",
    "\n",
    "    test_instances = []\n",
    "    valid_count, test_count = 0,0\n",
    "\n",
    "    print(f\"Reading test data from the GCS bucket\")\n",
    "    for root, _, files in os.walk(os.path.join(data_dir, test_subdir)):\n",
    "        for fname in files:\n",
    "            fpath = os.path.join(root, fname)\n",
    "            if fpath.endswith(\".txt\"):\n",
    "                if random.uniform(0,1.0)<0.5:\n",
    "                    valid_count += 1\n",
    "                    ml_use=\"validation\"\n",
    "                else:\n",
    "                    test_count += 1\n",
    "                    ml_use=\"test\"\n",
    "                \n",
    "                data = read_data(fpath)\n",
    "\n",
    "                instance = generate_single_instance(\n",
    "                    file_path=fpath, ml_use=ml_use\n",
    "                )\n",
    "                test_instances.append(instance.json(by_alias=True, ensure_ascii=False)+'\\n')\n",
    "\n",
    "    print(f\"\\tFound {valid_count} validation instances and {test_count} test instances\")\n",
    "\n",
    "    return train_instances, test_instances\n",
    "\n",
    "train_instances, test_instances = create_instances(os.path.join(\"data\", \"unzip_truncated\", \"aclImdb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNXy80sI2QmD"
   },
   "source": [
    "## Writing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "X2EXSwb7qOAM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(\"data\", \"train_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(train_instances)\n",
    "\n",
    "with open(os.path.join(\"data\", \"test_instances.jsonl\"), \"w\") as f:\n",
    "    f.writelines(test_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFME07xPn6e8",
    "outputId": "b492d2ab-aa8b-4c6b-f332-563debe3a59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voUxjdeb19zx"
   },
   "source": [
    "## Creating the bucket if doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvIsVSano7Gf",
    "outputId": "851c0644-4f98-410b-b072-58052c4d2055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket gs://tutorials-deeplearninghero-imdb-review-dataset doesn't exist. Creating a new one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BucketNotFoundException: 404 gs://tutorials-deeplearninghero-imdb-review-dataset bucket does not exist.\n",
      "Creating gs://tutorials-deeplearninghero-imdb-review-dataset/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "if gsutil ls $BUCKET_URI; then\n",
    "    echo \"Bucket ${BUCKET_URI} already exists.\";\n",
    "else\n",
    "    echo \"Bucket ${BUCKET_URI} doesn't exist. Creating a new one\"\n",
    "    gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NxQ_mtiq5Hre",
    "outputId": "e4075af9-37a3-429c-eb6c-678cdf3a688e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/train_instances.jsonl [Content-Type=application/octet-stream]...\n",
      "- [1 files][  2.8 MiB/  2.8 MiB]                                                \n",
      "Operation completed over 1 objects/2.8 MiB.                                      \n",
      "Copying file://data/test_instances.jsonl [Content-Type=application/octet-stream]...\n",
      "- [1 files][  2.8 MiB/  2.8 MiB]                                                \n",
      "Operation completed over 1 objects/2.8 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/train_instances.jsonl $BUCKET_URI\n",
    "!gsutil cp data/test_instances.jsonl $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
