{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d06e4b3-3520-417d-9e47-1a96baba0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpei8w5vz3\n",
      "Requirement already satisfied: setuptools in /app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.runfiles/pypi_setuptools/site-packages (59.4.0)\n",
      "Requirement already satisfied: pip in /nix/store/vzqny68wq33dcg4hkdala51n5vqhpnwc-python3-3.9.12/lib/python3.9/site-packages (22.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m ensurepip --upgrade\n",
    "# There're two versions of vicuna deltas v1.1 and v0. For minigpt-4 we should use v0 (only compatible up to 0.1.10) v1.1 would work but would produce useless outputs\n",
    "!python -m pip install --quiet fschat==0.1.10 sentencepiece gdown gitpython\n",
    "!python -m pip install --quiet omegaconf==2.3.0 iopath==0.1.10 timm==0.6.13 opencv-python-headless==4.7.0.72 decord==0.6.0 webdataset==0.2.48 accelerate==0.16.0 bitsandbytes==0.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb9f7e2-1b4a-4248-8df0-6788ebe817b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "import os\n",
    "import gdown\n",
    "import transformers\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba3cb9-9611-46a9-93dd-8815dc82099e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up Mini-GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036882bb-7550-4b67-991c-6234e3bd8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /.cache/huggingface/hub/models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0593272-592c-43de-b666-5a0547868403",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import LlamaTokenizer\n",
    "\n",
    "default_cache_ir = pathlib.Path(\"/.cache/huggingface/hub\")\n",
    "llama_space = \"decapoda-research\"\n",
    "llama_id = \"llama-7b-hf\"\n",
    "vicuna_space = \"lmsys\"\n",
    "vicuna_id = \"vicuna-7b-delta-v0\"\n",
    "\n",
    "def download_models():\n",
    "    llama_repo_id = f\"{llama_space}/{llama_id}\"\n",
    "    vicuna_repo_id = f\"{vicuna_space}/{vicuna_id}\"\n",
    "    tokenizer = transformers.LlamaTokenizer.from_pretrained(llama_repo_id)\n",
    "    transformers.LlamaTokenizer.from_pretrained(vicuna_repo_id)\n",
    "\n",
    "    # transformers.AutoModelForCausalLM.from_pretrained\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(llama_repo_id)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(vicuna_repo_id)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "download_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5a1a8f-0fd0-4cb2-aab5-fafe3012996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /.cache/huggingface/hub/models--lmsys--vicuna-7b-delta-v0/snapshots/f902a2f7e2ca5dfeedf40a0220320e50d2d4fa2a/tokenizer_config.json\n",
      "No fix needed\n",
      "Loading /.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/tokenizer_config.json\n",
      "No fix needed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def patch_tokenizer_config(default_cache_dir):\n",
    "    # Magic fix introduced in https://github.com/huggingface/transformers/issues/22222#issuecomment-1477171703\n",
    "    for space, repo in [(vicuna_space, vicuna_id), (llama_space, llama_id)]:\n",
    "        for path in  (default_cache_dir / f\"models--{space}--{repo}\").rglob(\"snapshots/*/tokenizer_config.json\"):\n",
    "            print(f\"Loading {path}\")\n",
    "            config = json.loads(open(path, \"r\").read())\n",
    "            if config[\"tokenizer_class\"] == \"LlamaTokenizer\":\n",
    "                print(\"No fix needed\")\n",
    "            else:\n",
    "                config[\"tokenizer_class\"] = \"LlamaTokenizer\"\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(config, f)\n",
    "\n",
    "patch_tokenizer_config(default_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352ee1b3-707a-46fb-b29f-91d7dfa8c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the base model from /.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/5f98eefcc80e437ef68d457ad7bf167c2c6a1348/\n",
      "Loading checkpoint shards: 100%|████████████████| 33/33 [00:06<00:00,  4.99it/s]\n",
      "Loading the delta from /.cache/huggingface/hub/models--lmsys--vicuna-7b-delta-v0/snapshots/f902a2f7e2ca5dfeedf40a0220320e50d2d4fa2a/\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:04<00:00,  2.25s/it]\n",
      "Applying the delta\n",
      "Applying delta: 100%|█████████████████████████| 323/323 [00:03<00:00, 85.23it/s]\n",
      "Saving the target model to ./vicuna-7b-v0\n"
     ]
    }
   ],
   "source": [
    "# Vicuna weights are deltas which needs to be applied on top of llama\n",
    "!python -m fastchat.model.apply_delta --base /.cache/huggingface/hub/models--decapoda-research--llama-7b-hf/snapshots/*/  --target ./vicuna-7b-v0  --delta /.cache/huggingface/hub/models--lmsys--vicuna-7b-delta-v0/snapshots/*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79aba508-1b58-4736-9fcc-343f0e78987f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae37576b5ea7416a8fb37f32b83c9f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0362db2a7ee546198f5a388458d8f259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa543b78fed84e768bf4a44c2ddf2d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353ff330f5684d018b144816c0a88f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/543 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddfeac1300d14a28907fc6a5fbf0bbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeae80b9be204600905ca52942d085be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06551ebdd6d434b9e570ea723706875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb8c97f8207493aae0ac0dc12bc7b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 2&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>transformers.LlamaTokenizer.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">\"wangrongsheng/MiniGPT-4-LLaMA-7B\"</span>)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2 transformers.AutoModelForCausalLM.from_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">\"wangrongsheng/MiniGPT-4-LLaMA-7B\"</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_transformers/site-packages/transformers/models/auto/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">auto_factory.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">471</span> in         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">468 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">469 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(config) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping.keys():                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">470 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model_class = _get_model_class(config, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._model_mapping)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>471 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model_class.from_pretrained(                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">472 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">473 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">474 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_transformers/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2629</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2626 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>init_contexts.append(init_empty_weights())                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2627 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2628 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ContextManagers(init_contexts):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2629 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>model = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(config, *model_args, **model_kwargs)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2630 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2631 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Check first if we are `from_pt`</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2632 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_keep_in_fp32_modules:                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_transformers/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">614</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">611 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">LlamaForCausalLM</span>(LlamaPreTrainedModel):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">612 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, config):                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">613 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>(config)                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>614 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model = LlamaModel(config)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">615 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">616 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">617 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_transformers/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">445</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">442 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab_size = config.vocab_size                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">443 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">444 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pad   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>445 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layers = nn.ModuleList([LlamaDecoderLayer(config) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _ <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(config.num   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">446 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">447 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">448 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gradient_checkpointing = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_transformers/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">445</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;listcomp&gt;</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">442 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.vocab_size = config.vocab_size                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">443 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">444 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pad   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>445 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.layers = nn.ModuleList([LlamaDecoderLayer(config) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _ <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(config.num   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">446 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">447 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">448 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gradient_checkpointing = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_transformers/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">256</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">253 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">254 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size = config.hidden_size                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">255 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.self_attn = LlamaAttention(config=config)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>256 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp = LlamaMLP(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">257 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hidden_size=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.hidden_size,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">258 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>intermediate_size=config.intermediate_size,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">259 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>hidden_act=config.hidden_act,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_transformers/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">153</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">150 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>().<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>()                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">151 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">152 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.down_proj = nn.Linear(intermediate_size, hidden_size, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>153 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.up_proj = nn.Linear(hidden_size, intermediate_size, bias=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">154 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.act_fn = ACT2FN[hidden_act]                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x):                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_torch/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">101</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__init__</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 98 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias = Parameter(torch.empty(out_features, **factory_kwargs))             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.register_parameter(<span style=\"color: #808000; text-decoration-color: #808000\">'bias'</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>101 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.reset_parameters()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">102 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">103 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">reset_parameters</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Setting a=sqrt(5) in kaiming_uniform is the same as initializing with</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_torch/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">107</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">reset_parameters</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Setting a=sqrt(5) in kaiming_uniform is the same as initializing with</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">105 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># https://github.com/pytorch/pytorch/issues/57109</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>107 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>init.kaiming_uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, a=math.sqrt(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>))                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">108 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>fan_in, _ = init._calculate_fan_in_and_fan_out(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>bound = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> / math.sqrt(fan_in) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> fan_in &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">runfiles/pypi_torch/site-packages/torch/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">init.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">412</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">kaiming_uniform_</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">409 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>std = gain / math.sqrt(fan)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">410 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>bound = math.sqrt(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3.0</span>) * std  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calculate uniform bounds from standard deviation</span>       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">411 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>412 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> tensor.uniform_(-bound, bound)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">413 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">414 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">415 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">kaiming_normal_</span>(                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 2>\u001b[0m:\u001b[94m2\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mtransformers.LlamaTokenizer.from_pretrained(\u001b[33m\"\u001b[0m\u001b[33mwangrongsheng/MiniGPT-4-LLaMA-7B\u001b[0m\u001b[33m\"\u001b[0m)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2 transformers.AutoModelForCausalLM.from_pretrained(\u001b[33m\"\u001b[0m\u001b[33mwangrongsheng/MiniGPT-4-LLaMA-7B\u001b[0m\u001b[33m\"\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_transformers/site-packages/transformers/models/auto/\u001b[0m\u001b[1;33mauto_factory.py\u001b[0m:\u001b[94m471\u001b[0m in         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mfrom_pretrained\u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m468 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m469 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mtype\u001b[0m(config) \u001b[95min\u001b[0m \u001b[96mcls\u001b[0m._model_mapping.keys():                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m470 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_class = _get_model_class(config, \u001b[96mcls\u001b[0m._model_mapping)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m471 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m model_class.from_pretrained(                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m472 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mpretrained_model_name_or_path, *model_args, config=config, **hub_kwargs,   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m473 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m474 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_transformers/site-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2629\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2626 \u001b[0m\u001b[2m│   │   │   \u001b[0minit_contexts.append(init_empty_weights())                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2627 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2628 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ContextManagers(init_contexts):                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2629 \u001b[2m│   │   │   \u001b[0mmodel = \u001b[96mcls\u001b[0m(config, *model_args, **model_kwargs)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2630 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2631 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Check first if we are `from_pt`\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2632 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m use_keep_in_fp32_modules:                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_transformers/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m:\u001b[94m614\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__init__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m611 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mLlamaForCausalLM\u001b[0m(LlamaPreTrainedModel):                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m612 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m__init__\u001b[0m(\u001b[96mself\u001b[0m, config):                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m613 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m(config)                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m614 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.model = LlamaModel(config)                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m615 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m616 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[94mFalse\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m617 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_transformers/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m:\u001b[94m445\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__init__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m442 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.vocab_size = config.vocab_size                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m444 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[96mself\u001b[0m.pad   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m445 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.layers = nn.ModuleList([LlamaDecoderLayer(config) \u001b[94mfor\u001b[0m _ \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(config.num   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m446 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m447 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m448 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.gradient_checkpointing = \u001b[94mFalse\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_transformers/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m:\u001b[94m445\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m<listcomp>\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m442 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.vocab_size = config.vocab_size                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m443 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m444 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[96mself\u001b[0m.pad   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m445 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.layers = nn.ModuleList([LlamaDecoderLayer(config) \u001b[94mfor\u001b[0m _ \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(config.num   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m446 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m447 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m448 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.gradient_checkpointing = \u001b[94mFalse\u001b[0m                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_transformers/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m:\u001b[94m256\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__init__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m253 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m254 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.hidden_size = config.hidden_size                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m255 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.self_attn = LlamaAttention(config=config)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m256 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.mlp = LlamaMLP(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m257 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_size=\u001b[96mself\u001b[0m.hidden_size,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m258 \u001b[0m\u001b[2m│   │   │   \u001b[0mintermediate_size=config.intermediate_size,                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m259 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_act=config.hidden_act,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_transformers/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_llama.py\u001b[0m:\u001b[94m153\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m__init__\u001b[0m                                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96msuper\u001b[0m().\u001b[92m__init__\u001b[0m()                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=\u001b[94mFalse\u001b[0m)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\u001b[94mFalse\u001b[0m)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m153 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.up_proj = nn.Linear(hidden_size, intermediate_size, bias=\u001b[94mFalse\u001b[0m)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.act_fn = ACT2FN[hidden_act]                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x):                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_torch/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mlinear.py\u001b[0m:\u001b[94m101\u001b[0m in \u001b[92m__init__\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.bias = Parameter(torch.empty(out_features, **factory_kwargs))             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.register_parameter(\u001b[33m'\u001b[0m\u001b[33mbias\u001b[0m\u001b[33m'\u001b[0m, \u001b[94mNone\u001b[0m)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m101 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.reset_parameters()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mreset_parameters\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[94mNone\u001b[0m:                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_torch/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mlinear.py\u001b[0m:\u001b[94m107\u001b[0m in \u001b[92mreset_parameters\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m104 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[0m            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[0m             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m106 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# https://github.com/pytorch/pytorch/issues/57109\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m107 \u001b[2m│   │   \u001b[0minit.kaiming_uniform_(\u001b[96mself\u001b[0m.weight, a=math.sqrt(\u001b[94m5\u001b[0m))                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.bias \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   │   │   \u001b[0mfan_in, _ = init._calculate_fan_in_and_fan_out(\u001b[96mself\u001b[0m.weight)                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   \u001b[0mbound = \u001b[94m1\u001b[0m / math.sqrt(fan_in) \u001b[94mif\u001b[0m fan_in > \u001b[94m0\u001b[0m \u001b[94melse\u001b[0m \u001b[94m0\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mrunfiles/pypi_torch/site-packages/torch/nn/\u001b[0m\u001b[1;33minit.py\u001b[0m:\u001b[94m412\u001b[0m in \u001b[92mkaiming_uniform_\u001b[0m                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m409 \u001b[0m\u001b[2m│   \u001b[0mstd = gain / math.sqrt(fan)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m410 \u001b[0m\u001b[2m│   \u001b[0mbound = math.sqrt(\u001b[94m3.0\u001b[0m) * std  \u001b[2m# Calculate uniform bounds from standard deviation\u001b[0m       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m411 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m412 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m tensor.uniform_(-bound, bound)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m413 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m414 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m415 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mkaiming_normal_\u001b[0m(                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notes\n",
    "# wangrongsheng/MiniGPT-4-LLaMA-7B seems much better than vicuna-7b-v1.1 (decapoda-research/llama-7b-hf + lmsys/vicuna-7b-delta-v1.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2a8fa-cfae-4058-9091-0ef9848b4259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ad74823-10ed-42fd-8cf0-a6f4f4d5195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R\n",
      "From (redirected): https://drive.google.com/uc?id=1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R&confirm=t&uuid=fab2f9e0-f31a-4e34-81c2-6697438c2530\n",
      "To: /root/Autmn2023/pretrained_minigpt4.pth\n",
      "100%|██████████| 37.9M/37.9M [00:00<00:00, 192MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pretrained_minigpt4.pth'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'pretrained_minigpt4.pth'\n",
    "gdown.download(\n",
    "    \"https://drive.google.com/file/d/1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R/view?usp=sharing\", output_path, fuzzy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ebfab2d-d000-4d59-9c96-9ebd2ae8e626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 34.4M    0 34.4M    0     0  20.0M      0 --:--:--  0:00:01 --:--:-- 22.0M\n",
      "Warning: Got more output options than URLs\n"
     ]
    }
   ],
   "source": [
    "!curl -LO https://github.com/Vision-CAIR/MiniGPT-4/archive/refs/heads/main.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9059319d-8729-4a87-a45a-9eb7a54b9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"main.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53880870-3b96-4baf-b35b-d75e9d4bea22",
   "metadata": {},
   "source": [
    "## Running Mini-GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42ec8c7-9dc2-408a-83f8-89ac8900d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "minigpt4_path = './MiniGPT-4/MiniGPT-4-main'\n",
    "if sys.path[-1] != minigpt4_path:\n",
    "    sys.path.append(minigpt4_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b62dbd-9541-47a6-a6a5-34d8cbd30ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e872005a5c8b4905bd3a457c853152f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f3e6aa5d584f8da3b4fce1458b5811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d24853af71b46dfb3b68ca943ec1164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b53d9c567e469d9e91e505ce5d7b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VIT Done\n",
      "Loading Q-Former\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee5ab6f2ffa40cead4c892c289c9ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf580e8075fb496e97112b04cc6e86ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLAMA Done\n",
      "Load 4 training prompts\n",
      "Prompt Example \n",
      "###Human: <Img><ImageHere></Img> Could you describe the contents of this image for me? ###Assistant: \n",
      "Load BLIP2-LLM Checkpoint: ./pretrained_minigpt4.pth\n"
     ]
    }
   ],
   "source": [
    "import argparse \n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.registry import registry\n",
    "\n",
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\")\n",
    "parser.add_argument('--cfg-path', help='')\n",
    "parser.add_argument('--options', nargs=\"+\",help='')\n",
    "parser.add_argument('--gpu-id', default=0, help='')\n",
    "args = parser.parse_args(\" --cfg-path ./MiniGPT-4/MiniGPT-4-main/eval_configs/minigpt4_eval.yaml\".split())\n",
    "\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac2cb32-b593-4a54-a2d1-44c7ef28c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from minigpt4.conversation.conversation import *\n",
    "\n",
    "\n",
    "class MiniGPT4Chat:\n",
    "    \n",
    "    def __init__(self, model, vis_processor, device='cuda:0'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.vis_processor = vis_processor\n",
    "        stop_words_ids = [torch.tensor([835]).to(self.device),\n",
    "                          torch.tensor([2277, 29937]).to(self.device)]  # '###' can be encoded in two different ways.\n",
    "        self.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "        self.conv, self.img_list = None, None\n",
    "        self.reset_history()\n",
    "        \n",
    "    def ask(self, text):\n",
    "        if len(self.conv.messages) > 0 and self.conv.messages[-1][0] == self.conv.roles[0] \\\n",
    "                and self.conv.messages[-1][1][-6:] == '</Img>':  # last message is image.\n",
    "            self.conv.messages[-1][1] = ' '.join([self.conv.messages[-1][1], text])\n",
    "        else:\n",
    "            self.conv.append_message(self.conv.roles[0], text)\n",
    "\n",
    "    def answer(self, max_new_tokens=300, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, max_length=2000):\n",
    "        self.conv.append_message(self.conv.roles[1], None)\n",
    "        embs = self.get_context_emb(self.img_list)\n",
    "\n",
    "        current_max_len = embs.shape[1] + max_new_tokens\n",
    "        if current_max_len - max_length > 0:\n",
    "            print('Warning: The number of tokens in current conversation exceeds the max length. '\n",
    "                  'The model will not see the contexts outside the range.')\n",
    "        begin_idx = max(0, current_max_len - max_length)\n",
    "\n",
    "        embs = embs[:, begin_idx:]\n",
    "\n",
    "        outputs = self.model.llama_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=self.stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=True if num_beams==1 else False,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        output_token = outputs[0]\n",
    "        if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "        if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "        output_text = self.model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "        output_text = output_text.split('###')[0]  # remove the stop sign '###'\n",
    "        output_text = output_text.split('Assistant:')[-1].strip()\n",
    "        self.conv.messages[-1][1] = output_text\n",
    "        return output_text, output_token.cpu().numpy()\n",
    "\n",
    "    def upload_img(self, image):\n",
    "        if isinstance(image, str):  # is a image path\n",
    "            raw_image = Image.open(image).convert('RGB')\n",
    "            image = self.vis_processor(raw_image).unsqueeze(0).to(self.device)\n",
    "        elif isinstance(image, Image.Image):\n",
    "            raw_image = image\n",
    "            image = self.vis_processor(raw_image).unsqueeze(0).to(self.device)\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            if len(image.shape) == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "            image = image.to(self.device)\n",
    "\n",
    "        image_emb, _ = self.model.encode_img(image)\n",
    "        self.img_list.append(image_emb)\n",
    "        self.conv.append_message(self.conv.roles[0], \"<Img><ImageHere></Img>\")\n",
    "        msg = \"Received.\"\n",
    "        return msg\n",
    "\n",
    "    def get_context_emb(self, img_list):\n",
    "        prompt = self.conv.get_prompt()\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "        assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "        seg_tokens = [\n",
    "            self.model.llama_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(self.device).input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [self.model.llama_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "        mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "        mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "        return mixed_embs\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.conv = Conversation(\n",
    "            system=\"Give the following image: <Img>ImageContent</Img>. \"\n",
    "                   \"You will be able to see the image once I provide it to you. Please answer my questions.\",\n",
    "            roles=(\"Human\", \"Assistant\"),\n",
    "            messages=[],\n",
    "            offset=2,\n",
    "            sep_style=SeparatorStyle.SINGLE,\n",
    "            sep=\"###\",\n",
    "        )\n",
    "        self.img_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30bc0514-f3ee-4870-879c-640e19b6b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_paths = [\n",
    "    \"./design_gpt4_data/thumbnails/EAE2rRgKUhE.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE3o0b5yas.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE5qZX9tk4.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE8FTO3j5E.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE8tVORyhU.jpg\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200d2725-9fb6-4b3a-ae20-87fda50ea95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b458b916a5e4eee8c99cb1b20291c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(HTML(value='Template ID: EAE2rRgKUhE', layout=Layout(grid_area='widget001')), Image(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Layout, Box, Image, VBox, GridspecLayout, HTML\n",
    "\n",
    "images = []\n",
    "template_ids = []\n",
    "for path in thumbnail_paths:\n",
    "    images.append(Image(value=open(path, \"rb\").read(), format=\"jpg\", width=256, height=256))\n",
    "    template_ids.append(pathlib.Path(path).stem)\n",
    "\n",
    "\n",
    "# can use height_ratios to control height of rows?\n",
    "grid = GridspecLayout(4, len(template_ids), height='300px')\n",
    "\n",
    "for i, (img, tid) in enumerate(zip(images, template_ids)):\n",
    "    grid[0, i] = HTML(value=f\"Template ID: {tid}\")\n",
    "    grid[1:4, i] = img\n",
    "        \n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e085a57-abcc-491b-8756-6203afd34613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./design_gpt4_data/thumbnails/EAE2rRgKUhE.jpg :\n",
      "The image is a view of a lake with mountains in the background, with the words \"find your balance\" overlaid on top. The theme of the design is nature and tranquility. The colors are primarily green and blue, which give off a calming and peaceful vibe. The design emits a sense of serenity and encourages the viewer to take a moment to reflect on their own balance in life.\n",
      "--------------------------------------------------\n",
      "The text is white, and the background is blue. The contrast between the two colors is high, as the text is white and the background is a darker shade of blue. This high contrast creates a clear and defined visual hierarchy, making the text stand out and draw the viewer's attention to it. The contrast also creates a sense of contrast between the natural, calming image of the lake and the words \"find your balance,\" which emphasizes the importance of finding balance in life.\n",
      "--------------------------------------------------\n",
      "The style of the text is professional. The words \"find your balance\" are written in a clear, simple, and straightforward font, which fits well with the theme of the image, which is nature and tranquility. The style of the text matches the theme of the design, as it is serious and focused on finding balance in life, which is in line with the peaceful and calming atmosphere of the image.\n",
      "--------------------------------------------------\n",
      "Yes, the design is visually striking. The high contrast between the white text and the dark blue background creates a clear visual hierarchy, making the text stand out and draw the viewer's attention to it. The contrast also creates a sense of contrast between the natural, calming image of the lake and the words \"find your balance,\" which emphasizes the importance of finding balance in life. The color palette of green and blue creates a serene and calming atmosphere, which is in line with the theme of nature and tranquility. Overall, the design is visually striking due to its clear visual hierarchy, contrast, and serene color palette.\n",
      "--------------------------------------------------\n",
      "This design would be suitable for an enterprise user or an educational audience. The high contrast and clear visual hierarchy of the design make it suitable for enterprise users, who may be looking for a professional and straightforward design. The serene and calming atmosphere of the image and text would also be suitable for an educational audience, such as a college or university, where students may be looking for inspiration and guidance on finding balance in their lives. Overall, the design is suitable for an audience that is looking for a professional and calming design that conveys a clear message about the importance of finding balance in life.\n",
      "--------------------------------------------------\n",
      "The design is not pedagogical and is not particularly suited for a classroom or children. The high contrast and clear visual hierarchy of the design, as well as the serious and professional tone of the text, would not be suitable for a classroom setting or children. Children and classrooms often require designs that are more visually engaging and playful, with a focus on learning and education. The serene and calming atmosphere of the image, while soothing, would not provide enough stimulation or education for a classroom or children. Overall, the design is not pedagogical and is not particularly suited for a classroom or children.\n",
      "==================================================\n",
      "./design_gpt4_data/thumbnails/EAE3o0b5yas.jpg :\n",
      "The image is a pancakes recipe with a pink background and a text overlay saying, \"Pancakes Recipe.\" The main objects are pancakes on a plate, and the theme is breakfast or brunch. The colors are bright and cheerful, giving off a warm and inviting vibe.\n",
      "--------------------------------------------------\n",
      "The text is white and the background is pink. The colors create a strong contrast, making the text stand out and making it easy to read.\n",
      "--------------------------------------------------\n",
      "The style of the text is professional. The text does match the theme of the design, which is a pancakes recipe.\n",
      "--------------------------------------------------\n",
      "Based on the outputs so far, the design is visually striking because of the strong contrast between the white text and the pink background, which makes the text easy to read and draws the viewer's attention to it. The color palette is also bright and cheerful, which creates a warm and inviting vibe. Overall, the design is well-balanced and visually appealing.\n",
      "--------------------------------------------------\n",
      "This design would be suitable for an average person, as it is visually appealing and easy to read. It would also be suitable for an enterprise user, as the professional tone of the text would be appropriate for a business setting. It could also be suitable for an educational setting, as the content is related to a recipe and would be informative for someone who is interested in cooking.\n",
      "--------------------------------------------------\n",
      "Based on the visual elements, this design is not particularly pedagogical or suited to a classroom or children. The text is not written in a way that is specifically geared towards children, and the color scheme and overall design are not particularly child-friendly. However, the design could still be used in a classroom setting if the content is appropriate for the intended audience.\n",
      "==================================================\n",
      "./design_gpt4_data/thumbnails/EAE5qZX9tk4.jpg :\n",
      "The main objects in this image are a woman wearing a white towel and smiling at the camera. The theme of this image is a relaxing spa experience, as the woman is in a bathrobe and has just finished a spa treatment. The colors of this image are soft and calming, with a pale beige background and the woman wearing a white towel. The overall vibe of this design is one of relaxation and rejuvenation.\n",
      "--------------------------------------------------\n",
      "The text in the image is white, and the background is a pale beige color. The contrast between the text and the background is strong, as the white text stands out against the slightly lighter beige background. This helps to draw the viewer's attention to the text, which is likely the main focus of the image.\n",
      "--------------------------------------------------\n",
      "The style of the text in this image is professional, as it is written in a clear, legible font and is likely meant to convey information about spa treatments. The text does match the theme of the design, as it is related to the relaxing and rejuvenating aspects of spa treatments. Overall, the style and theme of the text are well-matched in this image.\n",
      "--------------------------------------------------\n",
      "Based on the outputs so far, this is a visually striking design. The contrast between the white text and the pale beige background creates a strong visual impact, as the text stands out against the slightly lighter background. The color palette used in the image is also visually striking, as it includes a range of soft and calming colors that help to create a relaxing atmosphere. The overall contrast of the image is high, which helps to draw the viewer's attention to the text, which is likely the main focus of the design. The combination of the professional style of the text and the visually striking contrast of the text and background make this design visually striking.\n",
      "--------------------------------------------------\n",
      "This design would likely be suitable for an average person, as it is visually striking and likely to catch their attention. The design is likely meant to convey information about spa treatments, which may be of interest to people who are looking for ways to relax and rejuvenate. The professional style of the text would likely be appealing to this audience, as it conveys information in a clear and informative way. Overall, this design would be suitable for an average person who is interested in spa treatments and wants to learn more about them.\n",
      "--------------------------------------------------\n",
      "This design is not particularly pedagogical and would not be suitable for a classroom or children. The design is primarily focused on promoting spa treatments and conveying information about them to an adult audience. The professional style of the text and the relaxing atmosphere of the image would likely be of interest to adults, rather than children. Overall, this design is not pedagogical and would not be well-suited for a classroom or children.\n",
      "==================================================\n",
      "./design_gpt4_data/thumbnails/EAE8FTO3j5E.jpg :\n",
      "The image is a digital artwork featuring two people in traditional Arabian attire, standing and pointing at something in the distance. The theme of the image is Arabian culture and the vibe is one of mystery and intrigue. The colors used in the design are green and beige, which gives the image a calm and earthy feel.\n",
      "--------------------------------------------------\n",
      "The text in the image is white and the background is a light blue. Yes, the contrast between the text and the background is strong. The use of white text on a light blue background makes the text stand out and makes it easy to read. This is a good design choice as it draws attention to the important information being displayed.\n",
      "--------------------------------------------------\n",
      "The style of the text is professional, as it is written in a clear and legible font. The text is not comical or playful, but rather is serious and informative. The style of the text matches the theme of the design, as it is relevant to the image and conveys information about the people in the image.\n",
      "--------------------------------------------------\n",
      "Based on the outputs so far, this is not a visually striking design. The color palette is limited and there is not enough contrast between the text and the background. The overall contrast is low, as both the text and the background are light in color. Additionally, the image itself is not particularly visually striking, as it features only two people in traditional Arabian attire standing and pointing at something in the distance. To make the design more visually striking, a more contrasting color scheme could be used for the text and background, and more dynamic or interesting imagery could be incorporated into the design.\n",
      "--------------------------------------------------\n",
      "This design would be suitable for an average person. The image features two people in traditional Arabian attire, which may be interesting and informative for someone who is unfamiliar with Arabian culture. The text is clear and legible, which makes it easy for an average person to understand the information being conveyed. Additionally, the image is not too busy or cluttered, which makes it easy to take in and understand. Overall, this design would be appropriate for an average person who is interested in learning about Arabian culture.\n",
      "--------------------------------------------------\n",
      "This design is not pedagogical, suited for a classroom, or appropriate for children. The image features two adults in traditional Arabian attire, which may not be appropriate for a classroom setting or for children. Additionally, the text is not written in a way that is easily understandable for children, as it is written in a professional and serious tone. Finally, the image itself is not particularly interesting or engaging for children, as it features only two people standing and pointing at something in the distance. Overall, this design is not pedagogical, suited for a classroom, or appropriate for children.\n",
      "==================================================\n",
      "./design_gpt4_data/thumbnails/EAE8tVORyhU.jpg :\n",
      "The image appears to be an advertisement for a skincare product. The overall design is colorful and playful, with pink and purple flowers, green leaves, and white text. The theme of the image is beauty and self-care, as it promotes a skincare product and encourages people to take care of their skin. The mood of the image is cheerful and inviting.\n",
      "--------------------------------------------------\n",
      "The text is white and the background is pink. The contrast between the two colors is not very strong, as they are both muted and somewhat similar in tone. However, the white text provides a clear and readable contrast to the pink background. The combination of pink and white creates a soft and feminine look, which is appropriate for the skincare product being advertised.\n",
      "--------------------------------------------------\n",
      "The style of the text is professional, with clear and concise language that describes the product and its benefits. The text does not match the theme of the design, as it is not playful or colorful like the background. The text could be more visually appealing and better match the overall design if it were in a playful font or incorporated more color.\n",
      "--------------------------------------------------\n",
      "The design is visually striking in that it incorporates a playful and colorful background with clear and readable text, which provides a strong contrast to the background. The contrast between the text and the background creates a clear visual hierarchy, making the text easy to read and understand. The color palette is soft and feminine, which is appropriate for a skincare product, and the overall design is visually appealing and inviting. However, the design could be improved by incorporating more contrast between the text and the background, as well as more visual interest in the overall design. Additionally, the design could benefit from more playful elements, such as illustrations or graphics, to further enhance its visual appeal.\n",
      "--------------------------------------------------\n",
      "The design could be suitable for an average person who is interested in skincare products and wants to learn more about the benefits of using them. It could also be suitable for an enterprise user who is promoting a skincare product to consumers. However, the design may not be as suitable for an educational audience, as it does not contain any educational content or information, and may not be visually engaging enough to hold the attention of students or other educational users.\n",
      "--------------------------------------------------\n",
      "The design is not pedagogical, suited for a classroom or children. The design is primarily a promotional advertisement for a skincare product, and does not contain any educational content or information. The use of playful and colorful graphics may be appealing to children, but the design as a whole is not designed to facilitate learning or instruction. Therefore, this design is not pedagogical and is not suitable for a classroom or children.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Be careful if you import ipython Image object can conflict with this\n",
    "from PIL import Image\n",
    "\n",
    "# when I had all questions as a single text - and low temperatures it didn't say \"As an AI model I can't bla bla blla\"\n",
    "# start and follow up doesn't work much\n",
    "prompt_start = \"This is an instagram post design. Explain what are the main objects, what is the theme, based on colors what kind of a vibe does this design emit?\"\n",
    "prompts_followup = [       \n",
    "    \"What is the color of the text and the color of the background it's on? Do those colors create a strong contrast?\",\n",
    "    \"What is the style of the text, professional, comical or something else? Does the style of text match the them of the design?\",\n",
    "    \"Based on your outputs so far, is this a visually striking design. Explain why. Use topics such as text background contrast, overall contrast, color palette.\",\n",
    "    \"Pick and audience this design be suitable for (a) an average person, (b) enterprise user or (c) educational?\",\n",
    "    \"Analyse if this design pedogogical, suited to a classroom or children? \",\n",
    "    #\"Write 10 diverse instagram hashtags would you used to describe the style of this design. Do not mention specific objects in the design?\"\n",
    "]\n",
    "\n",
    "thumbnail_paths = [\n",
    "    \"./design_gpt4_data/thumbnails/EAE2rRgKUhE.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE3o0b5yas.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE5qZX9tk4.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE8FTO3j5E.jpg\", \n",
    "    \"./design_gpt4_data/thumbnails/EAE8tVORyhU.jpg\"\n",
    "]\n",
    "\n",
    "minigpt4 = MiniGPT4Chat(model, vis_processor)\n",
    "temperature = 0.9\n",
    "max_new_tokens = 200\n",
    "top_p = 0.9\n",
    "\n",
    "gpt4_outputs = []\n",
    "for path in thumbnail_paths:\n",
    "    full_output = []\n",
    "    minigpt4.upload_img(path)\n",
    "    minigpt4.ask(prompt_start)\n",
    "    out_start, _ = minigpt4.answer(\n",
    "        num_beams=1,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        top_p=top_p\n",
    "    )    \n",
    "    print(path,\":\")\n",
    "    print(out_start)\n",
    "    full_output.append(out_start)\n",
    "    for prompt in prompts_followup:\n",
    "        print(\"-\"*50)\n",
    "        minigpt4.ask(prompt)\n",
    "        out_follow, _ = minigpt4.answer(\n",
    "            num_beams=1,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        print(out_follow)\n",
    "        full_output.append(out_follow)\n",
    "    print(\"=\"*50)\n",
    "    minigpt4.reset_history()\n",
    "    gpt4_outputs.append(full_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3207825a-07f9-4730-9f2b-45cd0aabd426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b9adfda61e470e9c30a385b6d83e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xf…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Layout, Box, Image, VBox, GridspecLayout, HTML\n",
    "\n",
    "images = []\n",
    "template_ids = []\n",
    "for path in thumbnail_paths:\n",
    "    images.append(Image(value=open(path, \"rb\").read(), format=\"jpg\", width=256, height=256))\n",
    "    template_ids.append(pathlib.Path(path).stem)\n",
    "\n",
    "\n",
    "# can use height_ratios to control height of rows?\n",
    "grid = GridspecLayout(len(template_ids), 4)\n",
    "\n",
    "for i, (img, out) in enumerate(zip(images, gpt4_outputs)):\n",
    "    grid[i, 0] = img    \n",
    "    full_res = \"\"\n",
    "    for q, r in zip([prompt_start] + prompts_followup, out):\n",
    "        full_res += f\"<b>{q}</b><br/>{r}<br/>\"\n",
    "    grid[i, 1:] = HTML(value=full_res)\n",
    "    \n",
    "        \n",
    "display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c57f1e3-fa24-4f4c-a1f3-091e72305972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_thumbnails/EAE2rRgKUhE.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is a light blue.\n",
      "--------------------------------------------------\n",
      "The text color is black and the background color is dark green. The text is low contrast in this image.\n",
      "```\n",
      "==================================================\n",
      "sample_thumbnails/EAE3o0b5yas.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is yellow and the background color is orange. The text and background color combination is high contrast.\n",
      "    Therefore, the text is highly visible and readable in this image.</s><s>\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is pink. The text is high-contrast and well-designed in this image.</s><s>\n",
      "==================================================\n",
      "sample_thumbnails/EAE5qZX9tk4.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is beige. The text and background combination is high-contrast, making the text easy to read.</s><s>\n",
      "--------------------------------------------------\n",
      "\n",
      "==================================================\n",
      "sample_thumbnails/EAE8FTO3j5E.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is black and the background color is a light green.\n",
      "\n",
      "The text background combination is high-contrast in this image.</s><s>\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is light green. The contrast is high.\n",
      "==================================================\n",
      "sample_thumbnails/EAE8tVORyhU.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is blue.\n",
      "    Answer:\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is light pink. This is a high-contrast combination, which makes the text more readable.\n",
      "==================================================\n",
      "sample_thumbnails/EAEi-Hy31Ws.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is black and the background color is white. The text background combination is high-contrast. The contrast ratio of the text color and background color is over 1000:1. Answer: 1000:1.</s><s>\n",
      "--------------------------------------------------\n",
      "As an assistant, I can confirm that the text color is white and the background color is a light pink. The contrast between the two colors is high, which makes the text more readable and aesthetically pleasing. In terms of design, the combination of the white text on a light pink background is considered a good design choice.</s><s>\n",
      "==================================================\n",
      "sample_thumbnails/EAEs3-Md18A.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is blue. The text and background color combination is high-contrast, making the text easy to read. The image is a watercolor painting with a bicycle in the foreground and floral arrangements in the background.</s><s>\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is a light blue. The contrast is high, making the text easy to read.</s><s>\n",
      "==================================================\n",
      "sample_thumbnails/EAEyG9tZVc0.jpg :\n",
      "--------------------------------------------------\n",
      "The text color is white and the background color is yellow. The text and background color combination is low contrast.</s><s>\n",
      "--------------------------------------------------\n",
      "<Img>Text on background\n",
      "==================================================\n",
      "sample_thumbnails/EAFBw-ZDwJU.jpg :\n",
      "--------------------------------------------------\n",
      "<Img></s><s>\n",
      "--------------------------------------------------\n",
      "Question 1: The text color is white and the background color is green.\n",
      "    Question 2: The text background combination is a high contrast in this image. The white text stands out against the dark green background.</s><s>\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# when I had all questions as a single text - and low temperatures it didn't say \"As an AI model I can't bla bla blla\"\n",
    "# start and follow up doesn't work much\n",
    "prompts = [    \n",
    "    #\"Explain what are the main objects, what is the theme, based on colors what kind of a vibe does this design emit?\",\n",
    "    #\"What is the color of the text and the color of the background it's on? Is the text well highlighted and easy to read?\",\n",
    "    #\"Is the text light or dark? Is the background light or dark? Is the text well highlighted against the background and easy to read?\",\n",
    "    #\"Is the text readable in this background?\",\n",
    "    #\"Usually it's better to have highly contrastive text with background such as black and white or blue and yellow. Blue and green on the other hand is not contrastive. How does it apply here?\"\n",
    "    # <-- Below one works reasonably well --> \n",
    "    # \"Text is more readable when it constrasts with background such as black and white or blue and yellow. Blue and green on the other hand is not contrastive or readable. Is the text readable here?\",\n",
    "    #\"What is the style of the text, professional, comical or something else? Does the style of text match the them of the design or not?\",\n",
    "    \"\"\"Having complementary text color and a background color, such as black and white or blue and orange makes text higly contrastive, readable and pleasing. \n",
    "    Having both the text and the background in low contrast, such as yellow text with a white background makes text difficult to read. \n",
    "    Using shades of the same color for text and background, such as a green background and a green shade text, can make a design appear dull and unreadable. \n",
    "    Question 1: What the text color?\n",
    "    Question 2: What is the background color?\n",
    "    Question 3: Is the text background combination high-contrast or low-contrast in this image? \n",
    "    Answer:\"\"\",\n",
    "    \"\"\"Text is more readable and considered a good design when there's strong contrast: e.g. white text on a black background, red text on a yellow background, light blue text on a dark purple background, dark green text on a light pink background, Black text on a bright green background\n",
    "    Text is unreadable and considered dull and poor design when it is low contrast: e.g. Light gray text on a white background, Pale yellow text on a light green background, Dark blue text on a black background, Dark purple text on a dark green background, Dark gray text on a light blue background\n",
    "    Question 1: What is the text color and background color?\n",
    "    Question 2: Is the text background combination high-contrast or low-contrast in this image? \n",
    "    Answer:\n",
    "    \"\"\",\n",
    "    #\"What is the audience this design be suitable for (a) an average person or (b) enterprise user or (c) educational?\",\n",
    "    #\"Analyse if this design pedogogical, suited to a classroom or children or not? \",\n",
    "    #\"\"\"In a classroom setting, an effective educational visual design should be large enough to be visible to all students \n",
    "    #and be easily understood without requiring extensive explanation. \n",
    "    #It should also use a color scheme that is visually appealing and consistent with the subject matter being taught. \n",
    "    #Question 1: Does this design have a school related theme? \n",
    "    #Question 2: Is this design pedogogical, suited to a classroom or children or not?\n",
    "    #Answer: \"\"\",\n",
    "    #\"What are 10 diverse instagram hashtags would you used to describe the style of this design. Do not mention specific objects in the design?\"\n",
    "]\n",
    "\n",
    "thumbnail_paths = [\n",
    "    \"sample_thumbnails/EAE2rRgKUhE.jpg\", \n",
    "    \"sample_thumbnails/EAE3o0b5yas.jpg\", \n",
    "    \"sample_thumbnails/EAE5qZX9tk4.jpg\", \n",
    "    \"sample_thumbnails/EAE8FTO3j5E.jpg\", \n",
    "    \"sample_thumbnails/EAE8tVORyhU.jpg\",\n",
    "    \"sample_thumbnails/EAEi-Hy31Ws.jpg\",\n",
    "    \"sample_thumbnails/EAEs3-Md18A.jpg\",\n",
    "    \"sample_thumbnails/EAEyG9tZVc0.jpg\",\n",
    "    \"sample_thumbnails/EAFBw-ZDwJU.jpg\",\n",
    "]\n",
    "\n",
    "minigpt4 = MiniGPT4Chat(model, vis_processor)\n",
    "temperature = 0.95\n",
    "max_new_tokens = 100\n",
    "top_p = 0.9\n",
    "length_penalty=0.5\n",
    "\n",
    "gpt4_outputs = []\n",
    "for path in thumbnail_paths:\n",
    "    full_output = \"\"\n",
    "    \n",
    "    print(path,\":\")\n",
    "    for prompt in prompts:\n",
    "        print(\"-\"*50)\n",
    "        minigpt4.upload_img(path)\n",
    "        minigpt4.ask(prompt)\n",
    "        out, _ = minigpt4.answer(\n",
    "            num_beams=1,\n",
    "            temperature=temperature,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            top_p=top_p,\n",
    "            length_penalty=length_penalty\n",
    "        )            \n",
    "        print(out)            \n",
    "        full_output += \" \" + out\n",
    "        minigpt4.reset_history()\n",
    "        \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    gpt4_outputs.append(full_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "630b7ba6-6c07-415d-9690-04eb2f2274e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation(system='Give the following image: <Img>ImageContent</Img>. You will be able to see the image once I provide it to you. Please answer my questions.', roles=('Human', 'Assistant'), messages=[['Human', '<Img><ImageHere></Img>']], offset=2, sep_style=<SeparatorStyle.SINGLE: 1>, sep='###', sep2=None, skip_next=False, conv_id=None)\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import IPython.display as display\n",
    "## Read images from file (because this is binary, maybe you can find how to use ByteIO) but this is more easy\n",
    "img1 = open('image1.jpeg', 'rb').read()\n",
    "img2 = open('image2.jpeg', 'rb').read()\n",
    "## Create image widgets. You can use layout of ipywidgets only with widgets.\n",
    "## Set image variable, image format and dimension.\n",
    "wi1 = widgets.Image(value=img1, format='png', width=300, height=400)\n",
    "wi2 = widgets.Image(value=img2, format='png', width=300, height=400)\n",
    "## Side by side thanks to HBox widgets\n",
    "sidebyside = widgets.HBox([wi1, wi2])\n",
    "## Finally, show.\n",
    "display.display(sidebyside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beda7b58-8fdc-4270-b74e-46a20d2ad513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n",
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:18<00:00,  9.21s/it]\n",
      "Loading LLAMA Done\n",
      "Load 4 training prompts\n",
      "Prompt Example \n",
      "###Human: <Img><ImageHere></Img> Could you describe the contents of this image for me? ###Assistant: \n",
      "Load BLIP2-LLM Checkpoint: ./pretrained_minigpt4.pth\n",
      "Initialization Finished\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://04f234d5480077b379.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
      "/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.runfiles/pypi_transformers/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://04f234d5480077b379.gradio.live\n"
     ]
    }
   ],
   "source": [
    "!python MiniGPT-4/MiniGPT-4-main/demo.py --cfg-path MiniGPT-4/MiniGPT-4-main/eval_configs/minigpt4_eval.yaml  --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485093ae-fef1-41ec-8ff5-e9a7540c8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output from straight downloaded vicuna\n",
    "\n",
    "./design_gpt4_data/thumbnails/EAE2rRgKUhE.jpg :\n",
    "The image shows a mountain lake with a rocky terrain in the foreground and snow-covered mountains in the background. The theme is nature, with a focus on the serenity and tranquility of the scene. The colors are mostly shades of green, blue, and grey, giving a calm and peaceful vibe to the design.\n",
    "--------------------------------------------------\n",
    "The text is white and the background is a dark blue. The contrast between the text and the background is strong, making the text stand out and creating a clear visual hierarchy.\n",
    "--------------------------------------------------\n",
    "The style of the text is professional and informative, which matches the theme of the image which is a serene and peaceful mountain lake. The text is not comical.\n",
    "--------------------------------------------------\n",
    "Based on the outputs so far, the design is visually striking due to the contrast between the white text and the dark blue background. This creates a clear visual hierarchy and draws the eye to the text. Additionally, the mountain lake in the background adds depth and interest to the design. The color palette consists mainly of shades of green, blue, and grey, which creates a calm and peaceful atmosphere. Overall, the design is visually striking due to the contrast, visual hierarchy, and serene atmosphere created by the colors and image.\n",
    "--------------------------------------------------\n",
    "This design would be suitable for an enterprise user, as it has a professional and informative style and the content would be relevant to a business or organization. The visual hierarchy created by the contrast between the text and background, as well as the clean and simple design, would make the information easy to understand and digest for an enterprise audience. It could also be suitable for an educational audience, as the serene atmosphere and informative content would make it suitable for teaching or training materials. However, the design would not be as effective for an average person, as it may be too formal and professional for everyday use.\n",
    "--------------------------------------------------\n",
    "The design is not particularly pedagogical in nature and would not be suitable for a classroom or for children. It does not have any specific educational content or features, such as diagrams or interactive elements, that would make it suited for a classroom setting. Additionally, the serene atmosphere and professional style may not be engaging for children. Overall, the design is not pedagogical and would not be suitable for a classroom or children.\n",
    "==================================================\n",
    "./design_gpt4_data/thumbnails/EAE3o0b5yas.jpg :\n",
    "This is a recipe for pancakes with strawberries and whipped cream. The image shows a plate with pancakes stacked on top of each other, with strawberries and whipped cream on top. The colors used are pink, red, and white. The overall vibe of this design is sweet and decadent, perfect for a breakfast or brunch dish.\n",
    "--------------------------------------------------\n",
    "The text is white and the background is pink. The contrast between the two colors is strong. The white text stands out against the pink background, making it easy to read and understand.\n",
    "--------------------------------------------------\n",
    "The style of the text is professional. The theme of the design is a recipe for pancakes with strawberries and whipped cream. The style of the text, being professional, matches the theme of the design.\n",
    "--------------------------------------------------\n",
    "Based on my outputs so far, the design is visually striking due to the contrast between the text and the background. The white text on a pink background creates a high level of contrast, making it easy to read and understand. Additionally, the theme of the design, which is a recipe for pancakes, pairs well with the visual representation of the pancakes. Overall, the design is aesthetically pleasing and effectively conveys the information it is meant to.\n",
    "--------------------------------------------------\n",
    "This design would be suitable for an average person, as it is a simple and easy to understand recipe for pancakes. The language used is clear and the visual representation of the pancakes makes it easy to visualize the end result. It could also be suitable for an enterprise user, as the design is professional in appearance and the information it conveys is relevant to the theme of the pancakes. It would not be suitable for an educational audience as it is not specifically designed for learning or teaching purposes.\n",
    "--------------------------------------------------\n",
    "Based on the design, it is not specifically designed for a classroom or children. The language used is simple and easy to understand, but the visual representation of the pancakes may not be relatable to children. The theme of the recipe, which is pancakes, may not be particularly engaging for children. It would not be suitable for a classroom or for teaching children.\n",
    "==================================================\n",
    "./design_gpt4_data/thumbnails/EAE5qZX9tk4.jpg :\n",
    "The image is a woman wearing a white towel and smiling. The theme of the image is beauty and skincare. The colors used in the design are white, beige and brown. The design emits a relaxing and spa-like vibe.\n",
    "--------------------------------------------------\n",
    "The text in the image is white and the background is beige. The contrast between the colors is not very strong.\n",
    "--------------------------------------------------\n",
    "The style of the text is professional. The text \"Beauty tips\" is written in sans-serif font and appears to be in a clean and organized format, which aligns with the overall professional theme of the design. The text does match the theme of the design, which is beauty and skincare.\n",
    "--------------------------------------------------\n",
    "Based on my outputs so far, this design is visually striking. The contrast between the white text and beige background is not very strong, but it is still noticeable. The color palette consists of white, beige, and brown, which creates a warm and inviting feel. The overall contrast is good, which makes the text stand out from the background. The use of a professional font and clean format also adds to the overall professional look of the design. All these elements contribute to the visual appeal of the design.\n",
    "--------------------------------------------------\n",
    "The design would be suitable for an average person. The information being presented is easy to understand and the use of white text on a beige background makes it easy to read. The overall style of the design is professional and clean, which makes it accessible to most people. It could also be suitable for enterprise users, but the style may not be as appealing to them as it would be for an average person. The design may not be suitable for educational purposes, as it does not convey any specific information related to education or learning.\n",
    "--------------------------------------------------\n",
    "This design is not pedagogical, suited to a classroom, or child-friendly. The style of the design is more geared towards professionalism, and the content being presented is geared towards beauty and skincare for adults. The use of beige background may not be suitable for a classroom setting, as it may not be visually appealing to children. Additionally, the font used in the design is not child-friendly, making it difficult for children to read. Therefore, this design is not suitable for a classroom or children.\n",
    "==================================================\n",
    "./design_gpt4_data/thumbnails/EAE8FTO3j5E.jpg :\n",
    "The image is a promotional image for a restaurant. The main objects are the two people, one is pointing at something on the menu and the other is holding a plate with food on it. The theme is food and dining. The colors are green and white, which gives a fresh and clean look to the image. The vibe the design emits is one of excitement and enjoyment.\n",
    "--------------------------------------------------\n",
    "The text is white and the background is green. Yes, the contrast is strong. The use of white text on a green background creates a clear visual separation between the two, making the text easy to read and stand out. This contrast also helps to draw the attention of the viewer to the text.\n",
    "--------------------------------------------------\n",
    "The style of the text is professional. The text is written in a clean and clear font, which gives an impression of professionalism. The text matches the theme of the design, which is a restaurant, as it is promoting the restaurant and its menu items. The use of a professional tone in the text helps to convey trust and reliability, which is important in the food industry.\n",
    "--------------------------------------------------\n",
    "Based on the outputs so far, this is a visually striking design. The contrast between the green background and the white text creates a clear visual separation, making the text easy to read and stand out. The use of the green background gives the design a fresh and natural look, while the white text gives it a professional and clean look. The color palette of green and white creates a harmonious and balanced design. The overall contrast creates a vivid and attention-grabbing design, which makes the restaurant's menu items more appealing and enticing. The design effectively communicates the theme of the restaurant, which is dining, and creates a sense of excitement and enjoyment.\n",
    "--------------------------------------------------\n",
    "This design would be suitable for (c) an educational audience. The design is simple and easy to read, making it suitable for an educational setting where the audience may not have much prior knowledge of the restaurant or its menu items. The use of a professional tone in the text helps to convey trust and reliability, which is important in an educational setting where the audience may be looking for reliable information. The overall design is visually striking and attention-grabbing, which helps to engage the audience and make learning more enjoyable. The color palette is also appropriate for an educational setting, as green is often associated with learning and growth.\n",
    "--------------------------------------------------\n",
    "This design is suitable for a classroom or children. The design is simple and easy to read, making it suitable for a classroom setting where children may not have much prior knowledge of the restaurant or its menu items. The use of a professional tone in the text helps to convey trust and reliability, which is important in a classroom setting where children may be looking for reliable information. The overall design is visually striking and attention-grabbing, which helps to engage children and make learning more enjoyable. The color palette is also appropriate for a classroom or children's setting, as green is often associated with learning and growth. The design is not pedagogical in nature, but it can be used to teach children about different types of food and menu items.\n",
    "==================================================\n",
    "./design_gpt4_data/thumbnails/EAE8tVORyhU.jpg :\n",
    "This is an Instagram post design that highlights the benefits of using skincare products. The main objects are the words \"Beauty Tips without Surgery\" in a bold and bold font. The theme of the design is skincare, and the color scheme is pink and white. The design emits a relaxed and natural vibe.\n",
    "--------------------------------------------------\n",
    "The text color is pink and the background color is white. The colors create a strong contrast, which is effective in making the text stand out and draw attention. The use of contrast can help make the message more impactful and readable.\n",
    "--------------------------------------------------\n",
    "The style of the text is professional. The text is written in a clear and easy-to-read font, which is appropriate for a professional design. The theme of the design is skincare, which is consistent with the professional tone of the text. The design is aimed at conveying information about skincare products in a straightforward manner, so the professional style of the text is appropriate.\n",
    "--------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
