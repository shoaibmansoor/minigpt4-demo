{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90af5e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 30 05:22:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.65       Driver Version: 527.37       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   50C    P8     6W /  N/A |      0MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A        22      G   /Xwayland                       N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631d6fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate \n",
    "import transformers\n",
    "import json\n",
    "\n",
    "REPO_ID = \"decapoda-research/llama-7b-hf\"\n",
    "\n",
    "# Make sure you set this variable according to the environment\n",
    "environment = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4efe67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model.embed_tokens\": 0,\n",
      "    \"model.layers.0\": 0,\n",
      "    \"model.layers.1\": 0,\n",
      "    \"model.layers.2.self_attn\": 0,\n",
      "    \"model.layers.2.mlp.gate_proj\": 0,\n",
      "    \"model.layers.2.mlp.down_proj\": \"cpu\",\n",
      "    \"model.layers.2.mlp.up_proj\": \"cpu\",\n",
      "    \"model.layers.2.mlp.act_fn\": \"cpu\",\n",
      "    \"model.layers.2.input_layernorm\": \"cpu\",\n",
      "    \"model.layers.2.post_attention_layernorm\": \"cpu\",\n",
      "    \"model.layers.3\": \"cpu\",\n",
      "    \"model.layers.4\": \"cpu\",\n",
      "    \"model.layers.5\": \"cpu\",\n",
      "    \"model.layers.6\": \"cpu\",\n",
      "    \"model.layers.7\": \"cpu\",\n",
      "    \"model.layers.8\": \"cpu\",\n",
      "    \"model.layers.9.self_attn\": \"cpu\",\n",
      "    \"model.layers.9.mlp.gate_proj\": \"cpu\",\n",
      "    \"model.layers.9.mlp.down_proj\": \"cpu\",\n",
      "    \"model.layers.9.mlp.up_proj\": \"disk\",\n",
      "    \"model.layers.9.mlp.act_fn\": \"disk\",\n",
      "    \"model.layers.9.input_layernorm\": \"disk\",\n",
      "    \"model.layers.9.post_attention_layernorm\": \"disk\",\n",
      "    \"model.layers.10\": \"disk\",\n",
      "    \"model.layers.11\": \"disk\",\n",
      "    \"model.layers.12\": \"disk\",\n",
      "    \"model.layers.13\": \"disk\",\n",
      "    \"model.layers.14\": \"disk\",\n",
      "    \"model.layers.15\": \"disk\",\n",
      "    \"model.layers.16\": \"disk\",\n",
      "    \"model.layers.17\": \"disk\",\n",
      "    \"model.layers.18\": \"disk\",\n",
      "    \"model.layers.19\": \"disk\",\n",
      "    \"model.layers.20\": \"disk\",\n",
      "    \"model.layers.21\": \"disk\",\n",
      "    \"model.layers.22\": \"disk\",\n",
      "    \"model.layers.23\": \"disk\",\n",
      "    \"model.layers.24\": \"disk\",\n",
      "    \"model.layers.25\": \"disk\",\n",
      "    \"model.layers.26\": \"disk\",\n",
      "    \"model.layers.27\": \"disk\",\n",
      "    \"model.layers.28\": \"disk\",\n",
      "    \"model.layers.29\": \"disk\",\n",
      "    \"model.layers.30\": \"disk\",\n",
      "    \"model.layers.31\": \"disk\",\n",
      "    \"model.norm\": \"disk\",\n",
      "    \"lm_head\": \"disk\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show that when we do this neither GPU nor CPU memory increases\n",
    "config = transformers.AutoConfig.from_pretrained(REPO_ID)\n",
    "\n",
    "with accelerate.init_empty_weights():\n",
    "    fake_model = transformers.AutoModelForCausalLM.from_config(config)\n",
    "    \n",
    "device_map = accelerate.infer_auto_device_map(fake_model, max_memory={0: \"3GiB\", \"cpu\": \"6GiB\"})\n",
    "print(json.dumps(device_map, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824795cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This device map was generated using accelerator.infer_auto_device_map() function\n",
    "device_map = {\n",
    "    'model.embed_tokens': 0,\n",
    "     'model.layers.0': 0,\n",
    "     'model.layers.1': 0,\n",
    "     'model.layers.2': 0,\n",
    "     'model.layers.3': 0,\n",
    "     'model.layers.4': 0,\n",
    "     'model.layers.5': 0,\n",
    "     'model.layers.6': 0,\n",
    "     'model.layers.7': 0,\n",
    "     'model.layers.8': 0,\n",
    "     'model.layers.9': 0,\n",
    "     'model.layers.10': 0,\n",
    "     'model.layers.11': 0,\n",
    "     'model.layers.12': 0,\n",
    "     'model.layers.13': 0,\n",
    "     'model.layers.14': 'cpu',\n",
    "     'model.layers.15': 'cpu',\n",
    "     'model.layers.16': 'cpu',\n",
    "     'model.layers.17': 'cpu',\n",
    "     'model.layers.18': 'cpu',\n",
    "     'model.layers.19': 'cpu',\n",
    "     'model.layers.20': 'cpu',\n",
    "     'model.layers.21': 'cpu',\n",
    "     'model.layers.22': 'cpu',\n",
    "     'model.layers.23': 'cpu',\n",
    "     'model.layers.24': 'cpu',\n",
    "     'model.layers.25': 'cpu',\n",
    "     'model.layers.26': 'cpu',\n",
    "     'model.layers.27': 'cpu',\n",
    "     'model.layers.28': 'disk',\n",
    "     'model.layers.29': 'disk',\n",
    "     'model.layers.30': 'disk',\n",
    "     'model.layers.31': 'disk',\n",
    "     'model.norm': 'disk',\n",
    "     'lm_head': 'disk'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555efa2",
   "metadata": {},
   "source": [
    "## Loading the model memory efficiently\n",
    "\n",
    "* LLM.int8() quantization\n",
    "* Offloading: Uses GPU memory to the maximum, then CPU and finally memory-mapped chunks on disk\n",
    "  * How offloading works: https://huggingface.co/docs/accelerate/usage_guides/big_modeling\n",
    "\n",
    "Note: Fixing bitsandbytes issue on WSL: https://github.com/TimDettmers/bitsandbytes/issues/156#issuecomment-1474056975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92f1560b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/thushv89/anaconda3/envs/ml.torch did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('unix')}\n",
      "  warn(msg)\n",
      "/home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No GPU detected! Check your CUDA paths. Proceeding to load CPU-only library...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: WARNING! libcuda.so not found! Do you have a CUDA driver installed? If you are on a cluster, make sure you are on a CUDA machine!\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Loading binary /home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfce51de9eea455dbb8aca156220c9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(REPO_ID)\n",
    "\n",
    "# Check what happens when device_map = auto\n",
    "# This will fail as the model in FP32 precision cannot be fit on CPU\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "# This will fail as the model in FP32 precision cannot be fit on GPU\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\").to(\"cuda\")\n",
    "# This will also fail as the model cannot be fit on GPU fully even with the quantization\n",
    "# model = transformers.LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\", device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "if environment == \"local\":\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        REPO_ID, \n",
    "        device_map=device_map, \n",
    "        offload_folder=\"/tmp/.offload\",\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,\n",
    "    )\n",
    "elif environment == \"colab\":\n",
    "    model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "        REPO_ID, \n",
    "        device_map=\"auto\", \n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Environment can only be local/colab. Got {environment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0a2f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint in bytes: 10710692352\n"
     ]
    }
   ],
   "source": [
    "# without quantization memory footprint-> 27020779520 (~25.1GB)\n",
    "# The more weights on GPU the better the memory reduction\n",
    "# with quantization memory footprint -> 10710692352 (~10GB)\n",
    "print(f\"Memory footprint in bytes: {model.get_memory_footprint()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c562b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model.embed_tokens\": 0,\n",
      "    \"model.layers.0\": 0,\n",
      "    \"model.layers.1\": 0,\n",
      "    \"model.layers.2\": 0,\n",
      "    \"model.layers.3\": 0,\n",
      "    \"model.layers.4\": 0,\n",
      "    \"model.layers.5\": 0,\n",
      "    \"model.layers.6\": 0,\n",
      "    \"model.layers.7\": 0,\n",
      "    \"model.layers.8\": 0,\n",
      "    \"model.layers.9\": 0,\n",
      "    \"model.layers.10\": 0,\n",
      "    \"model.layers.11\": 0,\n",
      "    \"model.layers.12\": 0,\n",
      "    \"model.layers.13\": 0,\n",
      "    \"model.layers.14\": \"cpu\",\n",
      "    \"model.layers.15\": \"cpu\",\n",
      "    \"model.layers.16\": \"cpu\",\n",
      "    \"model.layers.17\": \"cpu\",\n",
      "    \"model.layers.18\": \"cpu\",\n",
      "    \"model.layers.19\": \"cpu\",\n",
      "    \"model.layers.20\": \"cpu\",\n",
      "    \"model.layers.21\": \"cpu\",\n",
      "    \"model.layers.22\": \"cpu\",\n",
      "    \"model.layers.23\": \"cpu\",\n",
      "    \"model.layers.24\": \"cpu\",\n",
      "    \"model.layers.25\": \"cpu\",\n",
      "    \"model.layers.26\": \"cpu\",\n",
      "    \"model.layers.27\": \"cpu\",\n",
      "    \"model.layers.28\": \"disk\",\n",
      "    \"model.layers.29\": \"disk\",\n",
      "    \"model.layers.30\": \"disk\",\n",
      "    \"model.layers.31\": \"disk\",\n",
      "    \"model.norm\": \"disk\",\n",
      "    \"lm_head\": \"disk\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check if it provided the full list of layers without device map \n",
    "print(json.dumps(model.hf_device_map, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a84f3",
   "metadata": {},
   "source": [
    "## Inferring with the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b64b56c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I would explain a blackhole to a 5 year old as a place where the laws of physics don't apply.\n",
      "I'm not sure I'd go that far.\n",
      "It took 69.4328759409982s to generate the sequence of 25 tokens (0.36005998111390614 tokens/s).\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Remember Llama is not instruction finetuned\n",
    "batch = tokenizer(\n",
    "    \"I would explain a blackhole to a 5 year old as\",\n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=False\n",
    ")\n",
    "\n",
    "# /home/thushv89/anaconda3/envs/ml.torch/lib/python3.9/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on meta. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('meta') before running `.generate()`.\n",
    "batch = {k: v for k, v in batch.items()}\n",
    "n_input_tokens = batch[\"input_ids\"].shape[-1]\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "generated = model.generate(batch[\"input_ids\"].to(\"cuda\"), max_length=n_input_tokens+25)\n",
    "t2 = time.perf_counter()\n",
    "print(tokenizer.decode(generated[0]))\n",
    "n_generated = generated.shape[-1]-batch[\"input_ids\"].shape[-1]\n",
    "print(f\"It took {t2-t1}s to generate the sequence of {n_generated} tokens ({n_generated/(t2-t1)} tokens/s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c998b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
